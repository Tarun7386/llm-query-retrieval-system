# LLM Document Question-Answering System

A production-grade Retrieval-Augmented Generation (RAG) service that turns any document (PDF, DOCX, TXT, E-mail, etc.) into a searchable knowledge base and answers natural-language questions with sourced, confidence-scored responses.

---

## ğŸŒ High-Level Architecture

User â†’ FastAPI â†’ Query Embedding
â†˜â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
Pinecone Vector DB â† Chunk Embeddings
â”‚
Top-k Relevant Chunks
â”‚
Adaptive Answer Extractor
â”‚
JSON Response



| Layer                 | Main Responsibilities | Key Tech |
|-----------------------|-----------------------|----------|
| **Document Ingestion**| Download, OCR, text extraction, cleaning | `pdfminer`, `tesseract`, `docx2txt` |
| **Chunking**          | 1,200-char chunks Â±150-char overlap, metadata tagging | Custom Python |
| **Embedding**         | 768-dim semantic vectors | Google Gemini `models/embedding-001` |
| **Vector Storage**    | Similarity search, namespace isolation | Pinecone |
| **Query Processor**   | Embed question, semantic search, re-rank | FastAPI async |
| **Answer Engine**     | Multi-strategy extraction + LLM generation (optional) | Custom â€œAdaptiveAnswerExtractorâ€ |
| **API Gateway**       | Auth, rate-limit, CORS, JSON API | FastAPI |

---

## âš™ï¸ Flow Pipeline

1. **Upload / URL Input**  
2. **Document Processor**  
    â€¢ Detect formatâ€ƒâ€¢ Convert to textâ€ƒâ€¢ Clean headers/footers  
3. **Chunk & Embed**  
    â€¢ Context-aware splittingâ€ƒâ€¢ Gemini embeddings  
4. **Vector Upsert**  
    â€¢ Store chunks + metadata in Pinecone  
5. **Query**  
    â€¢ Embed questionâ€ƒâ€¢ Retrieve top-k chunks (cos sim â‰¥ 0.45)  
6. **Adaptive Extraction**  
    â€¢ Semantic, numerical, structural, contextual strategies  
    â€¢ Confidence scoring â†’ best answer  
7. **Response**  
    â€¢ `{"answers": ["â€¦", "â€¦"]}` returned to caller  

---

## ğŸ—„ï¸ Project Structure

â”œâ”€â”€ api/
â”‚ â”œâ”€â”€ endpoints.py # FastAPI routes
â”‚ â””â”€â”€ auth.py # Token auth
â”œâ”€â”€ services/
â”‚ â”œâ”€â”€ document_processor.py # Ingestion & cleaning
â”‚ â”œâ”€â”€ vector_store.py # Pinecone wrapper
â”‚ â”œâ”€â”€ adaptive_extractor.py # Smart answer finder
â”‚ â””â”€â”€ gemini_service.py # Google Gemini calls
â”œâ”€â”€ models/
â”‚ â”œâ”€â”€ database_models.py # SQLAlchemy ORM
â”‚ â””â”€â”€ pydantic_models.py # API schemas
â”œâ”€â”€ database.py # Postgres engine
â”œâ”€â”€ config.py # Env settings
â”œâ”€â”€ main.py # Uvicorn entry-point
â””â”€â”€ README.md




---

## ğŸš€ Quick Start

python -m venv .venv && source .venv/bin/activate

pip install -r requirements.txt

cp .env.example .env # edit DB, Pinecone, Gemini keys

uvicorn main:app --host 0.0.0.0 --port 8000 --reload


---

## ğŸ”‘ Environment Variables

| Variable | Description |
|----------|-------------|
| `DATABASE_URL` | Postgres connection string |
| `PINECONE_API_KEY` / `PINECONE_ENV` | Pinecone credentials |
| `GEMINI_API_KEY` | Google Gemini key |
| `VECTOR_DIMENSION` | Embedding size (default 768) |
| `LOG_LEVEL` | `INFO`, `DEBUG`, etc. |

---

## ğŸ› ï¸ Tech Stack

- **FastAPI** â€¢ **Pydantic** â€¢ **SQLAlchemy**  
- **Google Gemini Embedding**  
- **Pinecone Vector DB**  
- **Docker & Poetry** for packaging  
- **Alembic** for migrations

---

## ğŸ“ˆ Performance

| Metric                        | Typical Value |
|-------------------------------|---------------|
| PDF processing (20 pages)     | 15-30 s |
| Query latency (top-k=5)       | 1-3 s |
| Answer accuracy (structured)  | 85-95 % |

---

## ğŸ§© Extensibility Roadmap

1. **Multi-modal** (tables, images)  
2. **Feedback loop** â†’ fine-tune extraction thresholds  
3. **Streaming chat UI** with WebSockets  
4. **Micro-services split** (ingest vs. query)  
5. **Model swapping** (OpenAI, Cohere, Llama-CPP)  

---

## ğŸ¤ Contributing

1. Fork & branch (`feat/<name>`)  
2. Run `pre-commit install` for linting  
3. Add tests (`pytest`)  
4. Open a PRâ€”describe changes clearly  

---


